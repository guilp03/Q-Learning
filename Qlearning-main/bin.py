import numpy as np
from connection import connect
from connection import get_state_reward

s = connect(2037)
n_actions = 3
n_states = 24

#tabela que guarda as recompensas de cada estado e ação
Q_table = [[-9.08877411e+01, -1.01567786e+02, -1.04174897e+02],
 [-1.66615224e+02, -9.78591304e+01, -1.04013421e+02],
 [-1.59269452e+02, -1.04355864e+02, -1.04083982e+02],
 [-1.68287371e+02, -1.04066850e+02, -9.48598699e+01],
 [-8.47045673e+01, -8.95715733e+01, -9.25574152e+01],
 [-1.44262710e+02, -9.01388394e+01, -9.09375784e+01],
 [-9.67336269e+01, -8.97707768e+01, -8.94868622e+01],
 [-8.93249538e+01, -8.95817775e+01, -8.91064144e+01],
 [-7.66309929e+01, -9.29156128e+01, -9.30177984e+01],
 [-1.42707816e+02, -9.06111233e+01, -9.23288350e+01],
 [-9.96043565e+01, -9.16918476e+01, -9.13907900e+01],
 [-1.61388871e+02, -9.20526000e+01, -8.98916600e+01],
 [-6.99044689e+01, -7.09967778e+01, -7.11696149e+01],
 [-1.40233564e+02, -7.22776140e+01, -7.20364098e+01],
 [-7.83262813e+01, -7.18675383e+01, -7.15222910e+01],
 [-7.07722428e+01, -7.11650854e+01, -7.11264833e+01],
 [-1.55118304e+02, -6.52062990e+01, -6.56120879e+01],
 [-1.57292973e+02, -6.50469701e+01, -6.46997405e+01],
 [-6.33246922e+01, -6.33679243e+01, -6.32896558e+01],
 [-6.12795806e+01, -6.35278878e+01, -6.28309669e+01],
 [-5.41812923e+01, -5.60014137e+01, -5.58728376e+01],
 [-5.83706675e+01, -5.76932930e+01, -5.80176990e+01],
 [-6.07896599e+01, -5.81294606e+01, -5.83453393e+01],
 [-5.87559540e+01, -5.85227981e+01, -5.83022720e+01],
 [-1.55551362e+02, -5.09131777e+01, -5.11448167e+01],
 [-1.50078089e+02, -5.06926234e+01, -5.07928339e+01],
 [-5.03486228e+01, -4.92613376e+01, -4.90906928e+01],
 [-4.79871883e+01, -4.94125052e+01, -4.94160164e+01],
 [-3.80876665e+01, -4.18762176e+01, -4.16745050e+01],
 [-4.35594725e+01, -4.30120006e+01, -4.32777581e+01],
 [-4.57076901e+01, -4.38763103e+01, -4.39718386e+01],
 [-4.41589516e+01, -4.40966661e+01, -4.31705259e+01],
 [-1.56877338e+02, -3.66338407e+01, -3.15840799e+01],
 [-1.35394770e+02, -3.76598694e+01, -2.27914021e+01],
 [-3.90269954e+01, -3.42529320e+01, -9.85406612e+00],
 [ 2.19753083e+00, -3.25711929e+01, -3.41750466e+01],
 [-1.19118857e+02, -2.48586674e+01, -3.22228311e+01],
 [-3.46885816e+01, -3.21249993e+01, -4.62277876e+01],
 [-3.23272458e+01, -3.36743662e+01, -2.86028445e+01],
 [ 7.72230828e+00, -3.27148888e+01, -3.25008743e+01],
 [-1.00301068e+02, -2.04014192e+01, -3.22118510e+01],
 [-3.63032325e+01, -3.10530679e+01, -3.97146697e+01],
 [-1.12536180e+02, -3.08085376e+01, -3.84923971e+01],
 [ 2.22801304e+01, -3.20844626e+01, -3.18400862e+01],
 [-7.14005534e+01, -2.01196514e+01, -1.43588569e+01],
 [-1.71316324e+01, -1.53841458e+01,  1.12094682e+01],
 [ 1.31885170e+02, -1.42652701e+01, -1.10739959e+01],
 [-9.95255499e+01,  8.64686813e+01, -1.52826399e+01],
 [-1.93493305e+01, -2.66598045e+01, -4.14933932e+00],
 [-7.84400740e+01,  2.63766136e+01, -2.52044279e+01],
 [ 1.63253558e+02, -2.22062399e+01, -1.81147181e+01],
 [-4.50154452e+01,  2.54993118e+00, -1.24523831e+01],
 [ 1.85506664e+01, -1.00000000e-01,  0.00000000e+00],
 [-1.68200426e+01,  1.80578271e+00,  0.00000000e+00],
 [-2.88100000e-01, -2.71000000e-01,  1.90703013e+02],
 [ 2.17089371e+02,  2.94077734e+01,  1.94158808e+01],
 [-7.75048118e+01, -7.28731377e+01, -7.28561530e+01],
 [-7.26610925e+01, -7.25818166e+01, -7.25596203e+01],
 [-1.56922645e+02, -7.28671903e+01, -7.31110386e+01],
 [-1.57696947e+02, -7.31459804e+01, -7.33601193e+01],
 [-6.99303283e+01, -5.89982307e+01, -5.89994431e+01],
 [-5.66711506e+01, -5.79612228e+01, -5.80009210e+01],
 [-1.35427487e+02, -5.84120075e+01, -5.83628776e+01],
 [-1.56077465e+02, -5.89044013e+01, -5.86302726e+01],
 [-5.06395697e+01, -4.41815726e+01, -4.42116751e+01],
 [-4.33710709e+01, -4.33148765e+01, -4.52897636e+01],
 [-1.31728500e+02, -4.33126900e+01, -4.31632138e+01],
 [-1.58067095e+02, -4.42120458e+01, -4.41695169e+01],
 [-1.65715925e+02, -9.41354348e+01, -9.42162171e+01],
 [-1.01810376e+02, -9.39364828e+01, -9.40825095e+01],
 [-1.62786429e+02, -9.46265164e+01, -9.45944432e+01],
 [-9.79387150e+01, -9.46466088e+01, -9.44455196e+01],
 [-1.54579741e+02, -7.03224887e+01, -7.02945579e+01],
 [-1.10051633e+02, -7.34132741e+01, -7.06879906e+01],
 [-1.80890878e+02, -8.01908984e+01, -7.05964968e+01],
 [-7.49569821e+01, -7.05617545e+01, -7.16237513e+01],
 [-1.71658986e+02, -6.15556355e+01, -5.39623505e+01],
 [-8.31623488e+01, -5.33541433e+01, -6.22161892e+01],
 [-1.74060362e+02, -5.26022637e+01, -5.23110417e+01],
 [-6.38210971e+01, -5.25088726e+01, -5.61369909e+01],
 [-1.75922854e+02, -3.81649059e+01, -4.11102267e+01],
 [-4.64989335e+01, -3.71404785e+01, -4.97651624e+01],
 [-1.47503380e+02, -3.66982533e+01, -3.65687153e+01],
 [-2.95778455e+01, -3.94734620e+01, -3.83986154e+01],
 [-1.59070073e+01, -2.01990436e+01, -2.00155028e+01],
 [-2.65214480e+01, -1.96738331e+01, -1.97192156e+01],
 [-1.26626002e+02, -3.53476839e+01, -2.01778455e+01],
 [-1.27421545e+02, -3.35559391e+01, -2.04397279e+01],
 [-1.25280463e+01, -2.85835464e+01, -1.21579384e+01],
 [-6.90949796e+01, -1.35328005e+01, -2.67088617e+01],
 [-1.71318137e+01, -1.39553490e+01, -2.14500464e+01],
 [-7.18138205e+01, -1.28894605e+01, -2.48725865e+01],
 [-1.00000000e-01,  1.98698143e+01, -1.00000000e-01],
 [-1.46975102e+01,  1.18333405e+00, -1.00000000e-01],
 [-2.00000000e-01, -1.63951860e-01,  1.04253862e+01],
 [ 1.28350857e+02, -1.00000000e-01,  6.16699596e-01]]

#número de vezes que ele vai chegar ao final
n_episodes = 100

#número de tentativas que ele tem a cada episode de chegar ao final
max_iter_episode = 1000

#probabilidade de ele explorar algo novo e não se atentar necessariamente ao melhor valor conhecido
exploration_prob = 0.0

#uma vez que ele tenha ja conseguido chegar ao final uma vez, a necessidade de explorar diminui
exploration_decreasing_decay = 0.0

#teoricamente existe um minimo de exploração que precisa ter
min_exploration_prob = 0.0

#discount factor
gamma = 0.99

#learning rate
alpha = 0.1

#quanto custou o episode
total_rewards_episode = []

for i in range(n_episodes):
    current_state = "0b00000000"
    done = False
    
    total_episode_reward = 0
    
    for j in range(max_iter_episode):
        action_names = ["jump", "left", "right"]
        if np.random.uniform(0, 1) < exploration_prob:
            action = np.random.choice(action_names)
            print("random")
        else:
            action = action_names[np.argmax(Q_table[int(current_state, 2)][:])]
            print("consciente")
        next_state, reward = get_state_reward(s, action)
        
        if reward == 300:
            done = True
        
        
        if done:
            break
        current_state = next_state
        #print(Q_table)
        print(i,j)
        print(exploration_prob)
    #exploration_prob = max(min_exploration_prob, np.exp(-exploration_decreasing_decay*i))
    total_rewards_episode.append(total_episode_reward)
print(Q_table)

