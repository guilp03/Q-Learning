import numpy as np
from connection import connect
from connection import get_state_reward

s = connect(2037)
n_actions = 3
n_states = 24

#tabela que guarda as recompensas de cada estado e ação
Q_table = [[-68.95073535560041, -110.70178420929071, -111.5743432146406], 
[-170.21309729286054, -110.28620793725995, -111.10561827996658], 
[-159.269452, -112.70309232385725, -112.40731923380218], 
[-168.287371, -113.7284543751043, -102.19206381292622], 
[-98.74049154244862, -53.4771274321322, -99.70448566918142], 
[-144.26271, -97.28439934032482, -97.50424331281533], 
[-96.7336269, -95.1121144198117, -90.25751827390143], 
[-38.14024560495449, -94.2785569129979, -93.08064062788935], 
[-90.3792574959145, -92.9156128, -93.99357277270423], 
[-142.707816, -92.65086604000358, -93.43869904866], 
[-100.26033720549233, -93.76440230703001, -92.35098534],
[-161.388871, -92.0526, -89.76524983643837], 
[-81.42286586294848, -81.23195144288981, -82.17295715017879],
[-140.233564, -82.25574738196721, -82.75640295520363],
[-84.99297768762001, -81.39596448204732, -82.1136010777312],
[-80.01659901014243, -80.78781394700209, -80.38798024664662], 
[-155.118304, -76.47694674190207, -76.48226770578431], 
[-157.292973, -74.87212635270211, -74.99689681426963], 
[-72.90678119139363, -71.88574301580778, -71.97190765884245],
[-70.07359195804804, -70.31641478105684, -72.30862250170286], 
[-60.348045806864135, -60.77279080689632, -60.312383942840725],
[-64.33346718158235, -62.39765087225873, -62.47598983473105],
[-63.455730706562576, -62.93856982066239, -62.841993575643365],
[-64.06619544993455, -63.847367202318004, -64.20726520998657],
[-160.52072142619343, -55.2675980703439, -55.55081155070138],
[-150.078089, -54.19770081179471, -53.48721803879462], 
[-65.7454559551783, -51.20373799819231, -51.732066343291216], 
[-49.78733259717477, -51.08139319105908, -50.65648945867113], 
[-38.394301441054914, -41.8762176, -41.674505], 
[-43.5594725, -43.2926983050354, -43.9937370097], 
[-45.7076901, -44.47368153712357, -43.9718386], 
[-44.1589516, -44.0966661, -43.86298328932689], 
[-156.877338, -36.6338407, -30.509714224034585], 
[-135.39477, -37.502399800879395, -20.990744812477697], 
[-39.0269954, -34.252932, -3.944987331481602], 
[5.338562591799279, -32.5711929, -34.1750466], 
[-119.118857, -20.79150930435475, -32.2228311], 
[-34.6885816, -31.8735074426, -46.2277876], 
[-32.3272458, -33.6743662, -20.388400906874832], 
[37.19563845988273, -32.368504361356486, -32.5008743], 
[-100.301068, 0.06023626788442549, -32.211851], 
[-36.3032325, -28.725462653125877, -39.7146697], 
[-112.53618, -31.4881264653592, -38.4923971], 
[99.57730302095956, -32.0844626, -31.8400862], 
[-71.4005534, -20.1196514, -6.253256957148509], 
[-17.1316324, -15.3841458, 61.448319229575894], 
[140.35994905722234, -6.279520394454757, -11.0739959], 
[-99.5255499, 135.60762366453415, -15.2826399], 
[-23.660885827534806, -18.182202984011596, -5.6566927159463045], 
[-90.09502955197145, -5.755068902291397, -25.2044279], 
[199.93145735618342, -22.2062399, -14.601601128125399], 
[-45.0154452, 51.89679681654364, -12.4523831], 
[25.715407078469468, -0.1, 0.07877248829], 
[-16.8200426, 2.585452187486309, 0.0], 
[-17.7297740346055, -0.271, 211.7070752676303],
[211.8921229951636, 62.34797828708108, 19.4158808], 
[-73.13268014323087, -78.21453805426263, -77.83483429769534], 
[-78.025569899928, -77.61308785338761, -76.73734634283275], 
[-156.922645, -79.40087243198573, -79.31416035588468], 
[-157.696947, -80.65003104811487, -80.64672637278414], 
[-69.9303283, -63.81290995694934, -63.666849630714964], 
[-60.91793111058113, -62.51183694903856, -62.393211847189974], 
[-135.427487, -64.04385267464214, -63.55257767829955], 
[-160.86135062654247, -65.17541583719772, -64.17368742966705], 
[-50.6395697, -48.03470516818539, -47.455003440715274], 
[-45.066963084891576, -46.01253502459209, -46.35081364899147], 
[-131.7285, -46.25510066024821, -46.71370401037947], 
[-158.067095, -47.53488966241012, -47.99261680919989], 
[-165.715925, -84.48702992479502, -102.45323111776949], 
[-102.48836750325246, -103.11168516425255, -103.25824385679074], 
[-162.786429, -103.69811216616985, -94.02526815384665], 
[8.8149948322461, -102.50932092068138, -100.48354501699075], 
[-154.579741, -74.47715726638965, -74.36948711308509], 
[-110.051633, -74.65508549814757, -74.97106919039577],
[-180.890878, -80.1908984, -76.62416274552227], 
[26.62658254478466, -75.50908597729202, -75.21668004434392], 
[-171.658986, -52.577843098914414, -70.99538785734764], 
[-83.1623488, -63.68156453026228, -63.98157800605908], 
[-175.97074188903278, -63.44375394357302, -49.76916847495563], 
[77.30178079005448, -64.35305562887257, -63.873533437461674], 
[-174.71215002494517, -32.53578111113884, -43.534296241224226], 
[-48.59978708819387, -43.88665762752996, -49.7651624], 
[-147.50338, -41.150754953033065, -31.85848558868889], 
[111.81817481404332, -41.90555781506666, -42.36821944217494], 
[129.97583212597954, -8.802836343246723, -22.521297725548784], 
[-26.521448, -21.47143479733133, -20.318539825429376], 
[-126.626002, -35.3476839, 3.812132865939751], 
[-146.17705678955417, -18.4277358543683, 123.60667879361073], 
[141.4723011585833, -27.100802651198638, -12.468269475807169], 
[-69.0949796, 70.90820190995983, -22.351295436057896], 
[-17.1318137, 18.94315083045766, -21.4500464], 
[-71.8138205, -8.276656704352467, -24.8725865], 
[23.139647930914553, 180.77230553622218, -0.07284992905], 
[-14.6975102, 19.37179129395043, -0.1], 
[-0.2, -0.16395186, 27.804185413931442], 
[194.70628511832243, -0.1, 16.903842142856316]]

#número de vezes que ele vai chegar ao final
n_episodes = 100

#número de tentativas que ele tem a cada episode de chegar ao final
max_iter_episode = 1000

#probabilidade de ele explorar algo novo e não se atentar necessariamente ao melhor valor conhecido
exploration_prob = 0.05

#uma vez que ele tenha ja conseguido chegar ao final uma vez, a necessidade de explorar diminui
exploration_decreasing_decay = 0.0

#teoricamente existe um minimo de exploração que precisa ter
min_exploration_prob = 0.0

#discount factor
gamma = 0.99

#learning rate
alpha = 0.1

#quanto custou o episode
total_rewards_episode = []

for i in range(n_episodes):
    current_state = "0b00000000"
    done = False
    
    total_episode_reward = 0
    
    for j in range(max_iter_episode):
        action_names = ["jump", "left", "right"]
        if np.random.uniform(0, 1) < exploration_prob:
            action = np.random.choice(action_names)
            print("random")
        else:
            action = action_names[np.argmax(Q_table[int(current_state, 2)][:])]
            print("consciente")
        next_state, reward = get_state_reward(s, action)
        
        if reward == 300:
            done = True
            
        Q_table[int(current_state, 2)][action_names.index(action)] = (1-alpha) * Q_table[int(current_state, 2)][action_names.index(action)] + alpha * (reward + gamma * max(Q_table[int(next_state, 2)]))
        total_episode_reward = total_episode_reward + reward
        
        if done:
            break
        current_state = next_state
        #print(Q_table)
        print(i,j)
        print(exploration_prob)
    #exploration_prob = max(min_exploration_prob, np.exp(-exploration_decreasing_decay*i))
    total_rewards_episode.append(total_episode_reward)
print(Q_table)

